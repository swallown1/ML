## 解释下随机森林?

- 随机森林 = bagging + cart
- 随机：数据采样随机 + 特征列随机
	- 数据随机，指的是又放回的boostrap采样
	- 每个节点上对特征的选择都是从全量的特征中进行采样，**不剔除以利用的**
	- 特征随机指的是在子树分裂的时候，随机选择部分特征进行计算gini值，进行节点分裂。
	由于这两次的随机采样，是的模型的方差小，模型的返还能力更强。
- 森林：多决策树组合：
	- 可分类，也可以回归，回归是对输出值进行简单平均
	- 分类是对输出值进行投票

## 随机森林用的是什么树？
Cart树

## 随机森林的生成过程？
- 单科决策树
	- 随机选择样本
	- 从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂
	- 不需要剪枝，直到该节点的所有训练样例都属于同一类

- 生成若干的决策树  各决策树之间没有明显的关联。

## 解释下随机森林节点的分裂策略？

Gini系数

在连续值上：对值进行排序，计算不同分割点的Gini值，找到最好的分割点

在离散值上就是正常的计算Gini值，选择Gini值最好的特征进行分割

## 随机森林的损失函数是什么？

- 分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益

- 回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae

## 为了防止随机森林过拟合可以怎么做?

- 增加树的数量
- 增加叶子结点的数据数量
- 随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高

## 随机森林特征选择的过程？

特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前
更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。
相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有
可无 - 通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱
分布（相当于重新洗牌） - 是使用uniform或者gaussian抽取随机值替换原特征

## RF为什么比Bagging效率高？
Bagging无随机特征，使得训练决策树时效率更低



## RF的参数有哪些，如何调参？

要调整的参数主要是 n_estimators和max_features

- n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间
	也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效
	果并不会很显著地变好。
- max_features是分割节点时考虑的特征的随机子集的大小。 
	这个值越低，方差减小得越多，但是偏差的增大也越多
	- 回归：max_features = n_features
	- 分类：max_features = sqrt(n_features)

其他参数中

- class_weight也可以调整正负样本的权重
- max_depth = None 和 min_samples_split = 2 结合，为不限制生成一个不修剪的完全树


## RF的优缺点 ？
- 优点：
	- 不同决策树可以由不同主机并行训练生成，效率很高
	- 随机森林算法继承了CART的优点
	- 将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题

- 缺点：
	- 取值划分比较多的特征容易对RF的拟合效果产生严重影响  