## 集成学习之决策树

---

在看完决策树部分之后，接下来在看到集成学习在决策树上的应用，主要学习Random Forest 、Adaboost 和GDBT这几个算法的基本原理，有时间在加上一些对sklearn中的方法进行使用。

### 集成学习

主要的集成学习框架有3种boosting 、 Bagging和Stacking。而Random Forest 、Adaboost 和GDBT就是通过这三种方式将树模型进行集成二得到的算法，所以在介绍树模型之前  先说一说这三种集成学习策略。

##### 1.Boosting

Boosting的思想主要是从错误中学习更正，得到更好的结果。

**思想：**

Boosting的思想就是：训练一对的若训练器，通过按照权重进行累加以后，得到一个能达到一定标准的强分类器。但是所分类器之间是按照时间顺序得到的，也就是说当前的弱分类器
是在前一个分类器和结果之前的误差基础上进行构建的，而且不同的弱化器都有一个不同的权重。当建立新的分类器不会显著提升准确率，就停止迭代。
最终在预测的时候才用加权的方式得到预测的结果。


##### 2.Bagging

Bagging的全称为Bootstrap(抽样) aggregating。其思想就是通过一堆弱学习器形成一个智囊团，每次通过智囊团投票的方式得到最后总预测结果。。

**思想：**

	首先，利用抽样的思想，对所有样本采样出一堆子训练集，在每个自训练集上学习一个若学习器，每次需要预测就通过这些学习器进行分别预测，
最后数目最多的类别为预测类别，对于回归的问题，最终使用所有学习器的结果的均值代表整个分类器的预测结果。所以Bagging和Boosting最大的区别就是  Bagging可以并行的训练分类器，但是Boosting只能按照顺序进行训练学习器


这里我们就会了解到，在说Cart树时，就提到过，这种单颗树的方式不稳定，如果数据集方式一点改变，那么模型将无法使用。但是如果采用Bagging的思想，那么
就没事，我们可以在训练一些新的弱化器，学习改变部分。因此当学习算法不稳定时，采用这种思想就比较好。

##### 3.Stacking
	
 Stacking 的思想就是利用全部的数据集进行训练模型，然后利用
 模型对每个训练样本进行预测，将预测值作为训练样本新的特征值，
 最终得到新的训练样本集，然后基于新的样本集训练得到模型，然后
 得到最终的预测结果。

**那么 我们再看看，用集成学习为什么要好一点呢？**
1. 首先，单个学习器可能并不能充分的学习数据里包含的规律，通过
	多个学习器的集成学习，可以有更好的一个泛化效果。
	
2. 假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；


### Random Forest
随机森林算法是用随机的方式建造多棵决策树，RF由多棵决策树组成，但是树之间没有什么联系。对于新的样本，每棵树都会给出一个预测结果
然后通过投票的方式来决定模型的最终输出(如果是回归型结果，就是将多棵树的结果取均值)。

**算法思想**：
随机森林算法采用的Bagging的算法思路，但是他在Bagging的思路是进行了改进。因此RF算法也是在Cart树作为弱学习器的基础构建的，其次这里的
cart树是做了改进的，由于RF算法在构建树的时候是选取部分特征（随机选择特征是指在每个节点在分裂过程中都是随机选择特征的（区别与每棵树随机选择一批特征）），在部分特征上选取最好特征进行划分样本的，所以这与cart有所不同。这样有助于进一步
提升其泛化能力。

我们假设其挑选的样本特征数目是$n_{sub}$，如果其值越小，对于RF来说会越健壮，但是在训练集上的的拟合效果会比较差，就是说$n_{sub}$越小，
模型的方差会减小，，在实际问题中，一般会通过交叉验证来得到合适的$n_{sub}$值。

**算法过程**：
对于样本集合$D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$,若学习器迭代T次，最终得到强学习器
对于每一次迭代
	-  随机选择样本，采用Bagging中的 bootstrap自助式采样方法，假设每轮采样m个样本
	-  对于每一轮样本，在随机采样$n_{sub}$ 个特征，在按照cart的算法进行构建
	-  最终构架T棵子树
模型的输出：如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。
如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。	

**优缺点**：
- 优点：
	1. 可以并行化训练模型，对于大数据集更高效
	2. 对于高纬度的数据，可以不用做特征筛选
	3. 由于采用了两次随机采样，训练出来的模型方差小，泛化能力强。
	4. RF的实现相比其他同类型算法实现简单，对于局部缺失特征不是很敏感。
* 缺点：
	1. 对于噪音大的数据集，容易过拟合
	2. 取值划分比较多的特征容易对RF的拟合效果产生严重影响  

**补充**：
基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。
	1.extra trees、2.Totally Random Trees Embedding(将数据从低维映射到高维，类似于SVM核函数)、3.Isolation Forest（以下简称IForest）是一种异常点检测的方法。它也使用了类似于RF的方法来检测异常点。 具体的原理可以在查资料
	
### AdaBoost

AdaBoost（Adaptive Boosting，自适应增强）,是集成学习Boosting方法的一个数的集成学习。
自适应体现在，前一个弱分类器的错误样本会在下一个训练器的训练样本中得到一个比较大的权重，似的下一个
学习器会更加重视这个错误样本，所有样本加权会给下一个训练器训练。每一次得到一个训练器，
直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

我们再来看看Boosting算法，首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，
使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。
然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，
得到最终的强学习器。

**算法思路**：

假设训练样本为：$T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$
训练集的在第k个弱学习器的输出权重为$D(k) = (w_{k1}, w_{k2}, ...w_{km}) ;\;\; w_{1i}=\frac{1}{m};\;\; i =1,2...m$

- 对于分类问题
	
	首先对于分类问题 我们计算第K个弱分类器$C_k(x)$在训练集上的加权误差率：
	$e_k = P(G_k(x_i) \neq y_i) = \sum\limits_{i=1}^{m}w_{ki}I(G_k(x_i) \neq y_i)$
	
	接着我们看弱学习器权重系数,对于二元分类问题，第k个弱分类器Gk(x)的权重系数为$\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k}$
	从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率小的弱分类器权重系数越大。
	
	更新新样本权重D。假设第k个弱分类器的样本集权重系数为$D(k) = (w_{k1}, w_{k2}, ...w_{km})$,则对应的第k+1个弱分类器的样本集权重系数为
	$$w_{k+1,i} = \frac{w_{ki}}{Z_K}exp(-\alpha_ky_iG_k(x_i))$$,这里Zk是规范化因子 $$Z_k = \sum\limits_{i=1}^{m}w_{ki}exp(-\alpha_ky_iG_k(x_i))$$
	从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)<0(因为对于二分类 1和-1两个类别)，导致样本的权重在第k+1个弱分类器中增大，如果分类正确，则权重在第k+1个弱分类器中减少.
	这就满足了错误的分类样本会得到一个大的权重。
	
	集合策略,Adaboost分类采用的是加权表决法，最终的强分类器为：$$f(x) = sign(\sum\limits_{k=1}^{K}\alpha_kG_k(x))$$

- 对于回归问题(以AdaBoost R2为例)
	对于回归问题的误差率的问题，对于第k个弱学习器，计算他在训练集上的最大误差$$E_k= max|y_i - G_k(x_i)|\;i=1,2...m$$
	
	然后计算每个样本的相对误差$$e_{ki}= \frac{|y_i - G_k(x_i)|}{E_k}$$
	
	这里是误差损失为线性时的情况，如果我们用平方误差，则$e_{ki}= \frac{(y_i - G_k(x_i))^2}{E_k^2}$
	如果是指数误差 则$e_{ki}= 1 - exp（\frac{-y_i + G_k(x_i))}{E_k}）$

	最终得到第k个弱学习器的 误差率$$e_k =  \sum\limits_{i=1}^{m}w_{ki}e_{ki}$$
	
	我们再来看看如何得到弱学习器权重系数α。这里有：$$\alpha_k =\frac{e_k}{1-e_k}$$
	
	对于更新更新样本权重D，第k+1个弱学习器的样本集权重系数为$$w_{k+1,i} = \frac{w_{ki}}{Z_k}\alpha_k^{1-e_{ki}}$$
	
	这里Zk是规范化因子$$Z_k = \sum\limits_{i=1}^{m}w_{ki}\alpha_k^{1-e_{ki}}$$
	
	结合策略，和分类问题稍有不同，采用的是对加权的弱学习器取权重中位数对应的弱学习器作为强学习器的方法，最终的强回归器为$$f(x) =G_{k^*}(x)$$
	
	其中，Gk∗(x)是所有ln1/αk,k=1,2,....K的中位数值对应序号k∗对应的弱学习器。　

**损失函数**：
	Adaboost 模型是加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。
	
	前向分步学习算法也好理解，我们的算法是通过一轮轮的弱学习器学习，利用前一个强学习器的结果和当前弱学习器来更新当前的强学习器的模型。也就是说，第k-1轮的强学习器为$$f_{k-1}(x) = \sum\limits_{i=1}^{k-1}\alpha_iG_{i}(x)$$
	
	而第k轮的强学习器为$$f_{k}(x) = \sum\limits_{i=1}^{k}\alpha_iG_{i}(x)$$
	
	上两式一比较可以得到fk(x)=fk−1(x)+αkGk(x)
	
	Adaboost损失函数为指数函数，即定义损失函数为$$\underbrace{arg\;min\;}_{\alpha, G} \sum\limits_{i=1}^{m}exp(-y_if_{k}(x))$$

	利用前向分步学习算法的关系可以得到损失函数为$$(\alpha_k, G_k(x)) = \underbrace{arg\;min\;}_{\alpha, G}\sum\limits_{i=1}^{m}exp[(-y_i) (f_{k-1}(x) + \alpha G(x))]$$
	
	令w′ki=exp(−yifk−1(x)), 它的值不依赖于α,G,因此与最小化无关，仅仅依赖于fk−1(x),随着每一轮迭代而改变。
	
	将这个式子带入损失函数,损失函数转化为$$(\alpha_k, G_k(x)) = \underbrace{arg\;min\;}_{\alpha, G}\sum\limits_{i=1}^{m}w_{ki}^{’}exp[-y_i\alpha G(x)]$$

	首先，我们求Gk(x).，可以得到$$G_k(x) = \underbrace{arg\;min\;}_{G}\sum\limits_{i=1}^{m}w_{ki}^{’}I(y_i \neq G(x_i))$$

	将Gk(x)带入损失函数，并对α求导，使其等于0，则就得到了$$\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k}$$

	其中，ek即为我们前面的分类误差率。$$e_k = \frac{\sum\limits_{i=1}^{m}w_{ki}^{’}I(y_i \neq G(x_i))}{\sum\limits_{i=1}^{m}w_{ki}^{’}} = \sum\limits_{i=1}^{m}w_{ki}I(y_i \neq G(x_i))$$
	
	最后看样本权重的更新。利用f_k(x)=f_k−1(x)+α_kG_k(x)和w′ki=exp(−yifk−1(x))，即可得：

	w′k+1,i=w′kiexp[−yiαkGk(x)]

**二分类问题过程**:
1) 初始化样本集权重为
	2) 使用具有权重Dk的样本集来训练数据，得到弱分类器Gk(x)
	3) 计算Gk(x)的分类误差率 ek
	4) 计算弱分类器的系数 αk
	5) 更新样本集的权重分布W_k+1
	2-5 一直重复k次  k为分类器个数
6) 构建最终分类器为：$f(x) = sign(\sum\limits_{k=1}^{K}\alpha_kG_k(x))$


对于Adaboost多元分类算法，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数
$$\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k} + log(R-1)$$

其中R为类别数。从上式可以看出，如果是二元分类，R=2，则上式和我们的二元分类算法中的弱分类器的系数一致。


**回归类问题过程**:

其过程是和二分类问题一样 就是用的公式不一样。

详细公式过程参看 [集成学习之Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)

**正则化**：
为了防止 Adaboost 过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长（learning rate）。对于前面的弱学习器的迭代
fk(x)=fk−1(x)+αkGk(x)

如果我们加上了正则化项，则有
fk(x)=fk−1(x)+ναkGk(x)

ν的取值范围为0<ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。

**Adaboost的主要优点有：**

1）Adaboost作为分类器时，分类精度很高

2）可以使用各种回归分类模型来构建弱学习器，非常灵活。

3） 不容易发生过拟合

**Adaboost的主要缺点有：**

1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重。




### GDBT
GDBT树（Gradient Boosting Decision Tree）指的是一种迭代的决策树，就是由一堆树通过Boosting方式组合成的一种算法，其泛化能力较强。

**算法思想**：
GBDT的含义就是用Gradient Boosting的策略训练出来的DT模型。GDBT的思想就是每一棵树学的是之前所有树结论和的残差，这个残差就是把一个加预测值后能得到真实值的累加量。
Gradient 指的其实就是一阶导数，对于基于残差的GDBT，其实是因为损失函数(平方损失)的负梯度(一阶导数)刚好就是残差的形式，所以基于残差的gbdt是一种特殊的gbdt模型，它的损失函数是平方损失函数，常用来处理回归类的问题。具体形式可以如下表示：
![](./image/m4.png)


模型的结果是一组回归分类树组合(CART Tree Ensemble)： T1 ... Tk 。其中 Tj 学习的是之前 j-1 棵树预测结果的残差，
模型最后的输出，是一个样本在各个树中输出的结果的和：
![](./image/p1.png)

**算法的基本流程**：

以GDBT的回归模型为例子 讲一下算法的基本流程：

输入是训练集样本T={(x,y1),(x2,y2),...(xm,ym)}， 最大迭代次数T, 损失函数L。

输出是强学习器f(x)

1.  初始化弱学习器
$$f_0(x) = \underbrace{arg\; min}_{c}\sum\limits_{i=1}^{m}L(y_i, c)$$

2. 对于每颗要学习的弱学习器有：
	
	a. 对样本i=1,2，...m，计算负梯度(计算负梯度的意义是为了在一阶倒数上来构建下一个弱学习器)
		![](./image/m8.png)
	
	
	b. 利用(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，
	其对应的叶子节点区域为Rtj,j=1,2,...,J。其中J为回归树t的叶子节点的个数。
	
	c. 对叶子区域j =1,2,..J,计算最佳拟合值(针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值ctj,所以其值得意义就是使得找出当前叶子节点构建最好的时候，其损失的最小值)
	![](./image/m9.png)
	
	d. 更新强学习器(通过ctj，拟合出来当前的这个弱化器，再将这个弱化器更新至前k-1个弱学习器上),下面公式第二部分也就是第k个若学习器
	
	![](./image/m10.png)

3. 得到强学习器f(x)的表达式：
![](./image/m.png)

**损失函数**：
基于残差的gbdt在解决回归问题上不算是一个好的选择，一个比较明显的缺点就是对异常值过于敏感。所以一般回归类的损失函数会用绝对损失或者huber损失函数来代替平方损失函数：
![](./image/m5.png)

对于分类损失函数其损失函数一般有对数损失函数和指数损失函数两种:

如果是指数损失函数，则损失函数表达式为
L(y,f(x))=exp(−yf(x))

如果是对数损失函数，分为二元分类和多元分类两种：

1. 对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：

L(y,f(x))=log(1+exp(−yf(x)))

其中y∈{−1,+1}。则此时的负梯度误差为
$$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$$

2. 多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：

$$L(y, f(x)) = -  \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$

其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为：
![](./image/m6.png)

　集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为
![](./image/m7.png)


**正则化(Shrinkage)**：
总结网上的好多资料，其实缩减(Shrinkage)其实就是其中一种正则化，和Adaboost中是一样的形式，都是为了防止过拟合。

Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。

![](./image/m11.png)

Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大。


当然也还有其他方式的正则化  通过子采样比例（subsample）和对于弱学习器即CART回归树进行正则化剪枝。

详细关注这篇内容[梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)


**优缺点**:

优点：

1) 可以灵活处理各种类型的数据，包括连续值和离散值。

2) 在相对少的调参时间情况下，预测的准确率也可以比较高。

3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。


缺点：
1)由于弱学习器之间存在依赖关系，难以并行训练数据



### 番外 

1. Boosting和Bagging的区别
	>1）样本选择上：
	>
	>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
	>
	>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
	>
	>2）样例权重：
	>
	>Bagging：使用均匀取样，每个样例的权重相等
	>
	>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
	>
	>3）预测函数：
	>
	>Bagging：所有预测函数的权重相等。
	>
	>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
	>
	>4）并行计算：
	>
	>Bagging：各个预测函数可以并行生成
	>
	>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。


2. GDBT 与 Adaboost 的对比

相同：

 - 都是 Boosting 家族成员，使用弱分类器；

 - 都使用前向分布算法；

不同：

 - 迭代思路不同：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），
	而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；
	
 - 损失函数不同：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；

### 参考资料

[机器学习中最最好用的提升方法：Boosting 与 AdaBoost](https://zhuanlan.zhihu.com/p/57689719)

[关于Boosting和Bagging的详解以及区别](https://zhuanlan.zhihu.com/p/57689719)

[Bagging 和 随机森林 刘建平Pinard](https://www.cnblogs.com/pinard/p/6156009.html)

[梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)

[决策树（中）——Random Forest、Adaboost、GBDT （非常详细）](https://zhuanlan.zhihu.com/p/86263786)


[机器学习-一文理解GBDT的原理-20171001](https://zhuanlan.zhihu.com/p/29765582)

